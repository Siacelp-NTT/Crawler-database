# Job Crawler Database System

A comprehensive two-tier database system for web scraping job postings with automated data transformation and analytics.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Crawlers  â”‚ (Your PC)
â”‚  (Python/Node)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP POST
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         VPS (DigitalOcean)          â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚      Raw API (Node.js)      â”‚   â”‚
â”‚  â”‚    Port 3000 (Internal)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚             â”‚                       â”‚
â”‚             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Raw Database (PostgreSQL)  â”‚   â”‚
â”‚  â”‚   Stores unprocessed data   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚             â”‚                       â”‚
â”‚             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Data Processor (Node.js)   â”‚   â”‚
â”‚  â”‚  Runs every 5 minutes       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚             â”‚                       â”‚
â”‚             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Clean Database (PostgreSQL) â”‚   â”‚
â”‚  â”‚  Normalized, ready for use  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Components

### 1. **Raw Database** (`/database-setup/raw_db/`)
- Fast insertion, flexible schema
- Stores all scraped data as-is
- Tracks processing status

### 2. **Clean Database** (`/database-setup/clean_db/`)
- Normalized schema with foreign keys
- Optimized for analytics queries
- Standardized data formats

### 3. **Raw API** (`/raw-api/`)
- RESTful API for data insertion
- Endpoints for job posts, companies, bulk uploads
- CSV file upload support
- Built with Express.js

### 4. **Data Processor** (`/data-processor/`)
- Automated ETL pipeline
- Platform-specific YAML configurations
- Salary parsing, location normalization
- AI fallback for complex fields
- Analytics dashboard

## ğŸš€ Quick Start

### Prerequisites

- **PostgreSQL** 15+ installed
- **Node.js** 18+ installed
- **npm** or **yarn**
- **VPS** (optional, for production deployment)

### Local Development Setup

#### 1. Clone the repository

```bash
git clone https://github.com/YOUR_USERNAME/Crawler-database.git
cd Crawler-database
```

#### 2. Setup Databases

```bash
# Create databases and users
psql -U postgres

-- In psql:
CREATE USER jobcrawler_user WITH PASSWORD 'your_password';
CREATE DATABASE jobcrawler_raw_db OWNER jobcrawler_user;
CREATE DATABASE jobcrawler_db OWNER jobcrawler_user;

-- Grant permissions
GRANT ALL PRIVILEGES ON DATABASE jobcrawler_raw_db TO jobcrawler_user;
GRANT ALL PRIVILEGES ON DATABASE jobcrawler_db TO jobcrawler_user;

\q
```

```bash
# Run schema scripts
psql -U jobcrawler_user -d jobcrawler_raw_db -f database-setup/raw_db/schema.sql
psql -U jobcrawler_user -d jobcrawler_db -f database-setup/clean_db/schema.sql
psql -U jobcrawler_user -d jobcrawler_db -f database-setup/clean_db/seed_data.sql
```

#### 3. Setup Raw API

```bash
cd raw-api
npm install
cp .env.example .env
# Edit .env with your database credentials
npm start
```

API will run on `http://localhost:3000`

#### 4. Setup Data Processor

```bash
cd data-processor
npm install
cp .env.example .env
# Edit .env with your database credentials

# Test with sample data
npm run test-data
npm run process-once

# Run analytics dashboard
npm run dashboard
```

## ğŸ“– Detailed Documentation

- [Database Setup Guide](./database-setup/README.md)
- [Raw API Documentation](./raw-api/README.md)
- [Data Processor Guide](./data-processor/README.md)
- [Deployment Instructions](./DEPLOYMENT.md)

## ğŸ”§ Configuration

### Environment Variables

**Raw API** (`.env`):
```env
DB_HOST=localhost
DB_PORT=5432
DB_NAME=jobcrawler_raw_db
DB_USER=jobcrawler_user
DB_PASSWORD=your_password
PORT=3000
MAX_FILE_SIZE=10485760
```

**Data Processor** (`.env`):
```env
RAW_DB_HOST=localhost
RAW_DB_NAME=jobcrawler_raw_db
CLEAN_DB_HOST=localhost
CLEAN_DB_NAME=jobcrawler_db
BATCH_SIZE=100
PROCESS_INTERVAL=*/5 * * * *
AI_ENABLED=false
```

## ğŸ“Š Analytics Queries

The system includes built-in analytics:

```bash
cd data-processor
npm run dashboard
```

Available reports:
- ğŸ’° Salary statistics by platform/experience
- ğŸ“ Jobs by location
- ğŸ¢ Platform performance
- ğŸ”¥ Trending job titles
- ğŸ“ˆ Hiring trends

## ğŸ§ª Testing

```bash
# Test Raw API
cd raw-api
npm test

# Test Data Processor
cd data-processor
npm run test-data      # Insert test data
npm run status         # Check processing status
npm run process-once   # Run processor once
```

## ğŸ“¡ API Endpoints

### POST `/api/jobposts`
Insert a single job posting

### POST `/api/companies`
Insert a company

### POST `/api/jobposts/bulk`
Bulk insert via JSON array

### POST `/api/upload/csv`
Upload CSV file

### GET `/api/health`
Health check

See [API Documentation](./raw-api/README.md) for details.

## ğŸš€ Production Deployment

1. **VPS Setup** (DigitalOcean, AWS, etc.)
2. **Install PostgreSQL**
3. **Deploy databases**
4. **Deploy Raw API** with PM2
5. **Deploy Data Processor** with PM2
6. **Setup Nginx** reverse proxy
7. **Configure firewall**

See [DEPLOYMENT.md](./DEPLOYMENT.md) for step-by-step guide.

## ğŸ“ˆ System Status

```bash
# Check Raw API
curl http://localhost:3000/api/health

# Check processor status
cd data-processor
npm run status

# View logs
npm run logs
```